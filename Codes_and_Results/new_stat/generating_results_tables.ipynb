{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_number_of_features_from_file(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            match = re.search(r\"number_of_features:\\s*(\\d+)\", line)\n",
    "            if match:\n",
    "                return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "filename = \"Results/main_log.txt\"\n",
    "number_of_features = extract_number_of_features_from_file(filename)\n",
    "print(number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating Mean and Max Results Tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = f\"FinalResults\"\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "def round_2(number):\n",
    "    return np.round(number, 2)\n",
    "\n",
    "random_state_numbers = [7, 42, 75, 101, 216]\n",
    "categorization_rules = \"R2B2\"\n",
    "preprocessing_methods = [\"MinMax\", \"Standard\"]\n",
    "target_columns = [\"r_dicho\", \"b_dicho\"]\n",
    "results_path = f'Results/3_Algorithms_Results'\n",
    "unWanted_columns = [\n",
    "    \"best_param_fold_1\",\"best_param_fold_2\",\n",
    "    \"best_param_fold_3\",\"best_param_fold_4\",\n",
    "    \"best_param_fold_5\"\n",
    "]\n",
    "desired_columns_order = [\n",
    "    \"Scaling\", \"Alg\",\n",
    "    \"Acc Cv\", \"Acc Te\",\n",
    "    \"Sp\", \"Se\", \"MCC\", \"F1\",\n",
    "    \"AUC-ROC\", \"AUPR\"\n",
    "]\n",
    "new_algorithms_names = [\"DT\", \"LR\", \"RF\", \"SVM\", \"GB\"]\n",
    "\n",
    "for target_column in target_columns:\n",
    "    mean_df_all = pd.DataFrame()\n",
    "    max_df_all = pd.DataFrame()\n",
    "    for preprocessing_method in preprocessing_methods:\n",
    "        final_df = pd.DataFrame()\n",
    "        for random_state in random_state_numbers:\n",
    "            tmp_path = f'{results_path}/{target_column}/{random_state}_{preprocessing_method}'\n",
    "            tmp_resultFile = pd.read_csv(f'{tmp_path}/results_df.csv')\n",
    "            final_df = pd.concat([final_df, tmp_resultFile], ignore_index=True)\n",
    "          \n",
    "        \n",
    "        tmp_mean_df = final_df.groupby(['Algorithm']).mean(numeric_only=True).drop(columns='Number_of_Features').reset_index()\n",
    "        tmp_std_df = final_df.groupby(['Algorithm']).std(numeric_only=True).drop(columns='Number_of_Features').reset_index()\n",
    "        \n",
    "        final_mean_df = pd.DataFrame({'Algorithm': tmp_mean_df['Algorithm'].values.tolist()})\n",
    "        for column in tmp_std_df.columns:\n",
    "            if \"_mean\" in column:\n",
    "                column_value = []\n",
    "                for i in range(len(tmp_mean_df)):\n",
    "                    new_value = f'{round_2(tmp_mean_df.loc[i, column])} \\u00B1 {round_2(tmp_std_df.loc[i, column])}'\n",
    "                    column_value.append(new_value)\n",
    "                final_mean_df[column] = column_value\n",
    "\n",
    "        final_mean_df.insert(0, \"preprocessing_method\", preprocessing_method)\n",
    "\n",
    "\n",
    "        tmp_max_df = final_df.groupby(['Algorithm']).max().drop(columns='Number_of_Features').reset_index()\n",
    "\n",
    "        final_max_df = pd.DataFrame({'Algorithm': tmp_max_df['Algorithm'].values.tolist()})\n",
    "        for column in tmp_max_df.columns:\n",
    "            if \"_mean\" in column:\n",
    "                column_value = []\n",
    "                for i in range(len(tmp_max_df)):\n",
    "                    Number_of_Features = final_df[\n",
    "                        (final_df[column] == tmp_max_df.loc[i, column])&(final_df['Algorithm']==tmp_max_df.loc[i, 'Algorithm'])\n",
    "                    ]['Number_of_Features'].to_list()[0]\n",
    "                    new_value = (round_2(tmp_max_df.loc[i, column]), Number_of_Features)                         \n",
    "                    column_value.append(new_value)\n",
    "                final_max_df[column.replace(\"_mean\",\"_max\")] = column_value\n",
    "\n",
    "        final_max_df.insert(0, \"preprocessing_method\", preprocessing_method)\n",
    "        \n",
    "        mean_df_all = pd.concat([mean_df_all, final_mean_df])  \n",
    "        max_df_all = pd.concat([max_df_all, final_max_df])\n",
    "    \n",
    "    \n",
    "    mean_df_all.columns = desired_columns_order\n",
    "    max_df_all.columns = desired_columns_order\n",
    "\n",
    "    mean_df_all.to_csv(f'{path}/{target_column}_mean.csv', index=False)\n",
    "\n",
    "    max_df_all.to_csv(f'{path}/{target_column}_max.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drowning AUC-ROC and AUPR Chart\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "path = f\"FinalResults\"\n",
    "if not os.path.isdir(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "# Set the custom font settings for scientific paper publications\n",
    "mpl.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.size': 18,\n",
    "    'font.weight': 'normal',\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'legend.fontsize': 14,\n",
    "})\n",
    "plt.close('all')\n",
    "\n",
    "random_state_numbers = [7, 42, 75, 101, 216]\n",
    "categorization_rules = \"R2B2\"\n",
    "preprocessing_methods = [\"MinMax\", \"Standard\"]\n",
    "target_columns = [\"r_dicho\", \"b_dicho\"]\n",
    "results_path = f'Results/3_Algorithms_Results'\n",
    "wanted_columns = [\n",
    "    'Algorithm', 'Number_of_Features',\n",
    "    'auc_roc_mean', 'aupr_mean'\n",
    "]\n",
    "metric_cols = ['auc_roc_mean', 'aupr_mean']\n",
    "desired_columns_order = [\n",
    "    \"Scaling\", \"Alg\",\n",
    "    \"AUC-ROC\", \"AUPR\"\n",
    "]\n",
    "al_names = [\"LR\", \"GB\"]\n",
    "\n",
    "# Define the line styles for each line\n",
    "line_styles = [\n",
    "            (0, (1, 0)),      # solid\n",
    "            (0, (1, 1)),      # dotted\n",
    "            (0, (5, 1)),      # dashed\n",
    "            (0, (3, 1, 1, 1)), # dashdot\n",
    "            (0, (3, 1, 1, 1, 1, 1))  # densely dashdotdotted\n",
    "        ]\n",
    "marker_styles = [\"o\", \"^\", \"p\", \"s\", \"D\"]\n",
    "\n",
    "for target_column in target_columns:\n",
    "    for preprocessing_method in preprocessing_methods:\n",
    "        df_list = []\n",
    "        for random_state in random_state_numbers:\n",
    "            tmp_path = f'{results_path}/{target_column}/{random_state}_{preprocessing_method}'\n",
    "            tmp_resultFile = pd.read_csv(f'{tmp_path}/results_df.csv')\n",
    "            df_list.append(tmp_resultFile[wanted_columns])\n",
    "        \n",
    "        final_df = pd.concat(df_list)\n",
    "        tmp_mean_df = final_df.groupby(['Algorithm', 'Number_of_Features']).mean().reset_index()\n",
    "        # Set the index of the DataFrame as a MultiIndex based on 'Algorithm' and 'Number_of_Features'\n",
    "        tmp_mean_df.set_index(['Algorithm', 'Number_of_Features'], inplace=True)\n",
    "        \n",
    "        for metric in metric_cols:\n",
    "            fig = plt.figure(figsize=(16, 9))\n",
    "            gs = GridSpec(1, 1)\n",
    "\n",
    "            ax_scores = fig.add_subplot(gs[0, 0])\n",
    "            \n",
    "            # Plot each series with the corresponding line style\n",
    "            for i, col in enumerate(tmp_mean_df[metric].unstack(level=0).columns):\n",
    "                ax_scores.plot(\n",
    "                    tmp_mean_df[metric].unstack(level=0).index,\n",
    "                    tmp_mean_df[metric].unstack(level=0)[col],\n",
    "                    linestyle=line_styles[i % len(line_styles)],  # Cycle through the line styles\n",
    "                    marker=marker_styles[i % len(marker_styles)],  # Cycle through the marker styles\n",
    "                    label=col\n",
    "                )\n",
    "                \n",
    "            # show grid lines\n",
    "            tick_positions = np.arange(0, number_of_features + 1, 10)\n",
    "            tick_positions = np.insert(tick_positions, 1, 1)\n",
    "            if tick_positions[-1] != number_of_features:\n",
    "                tick_positions = np.append(tick_positions, number_of_features)\n",
    "\n",
    "            ax_scores.set_xticks([i for i in range(1, number_of_features+1)])\n",
    "            ax_scores.set_xticklabels(['' if i not in tick_positions else str(i) for i in range(1, number_of_features+1)])\n",
    "\n",
    "            title = metric.replace(\"_\", \" \").title()\n",
    "            if \"Aupr\" in title:\n",
    "                title = title.replace(\"Aupr\",\"AUPR\")\n",
    "            if \"Auc Roc\" in title:\n",
    "                title = title.replace(\"Auc Roc\",\"AUC-ROC\")\n",
    "\n",
    "            # ax_scores.set_title(title)\n",
    "            ax_scores.set_xlabel('Number of Features')\n",
    "            ax_scores.set_ylabel('Score (%)')\n",
    "            ax_scores.grid(True)\n",
    "            \n",
    "            # show the legend\n",
    "            ax_scores.legend(loc='upper left')\n",
    "\n",
    "            # show/save the plot\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                f\"{path}/{target_column}_{preprocessing_method}_{metric}.png\",\n",
    "                bbox_inches='tight', facecolor='white', dpi=120\n",
    "            )\n",
    "            # plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Drowning AUC-ROC and AUPR Final Bar-Chart\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "# Set the custom font settings for scientific paper publications\n",
    "mpl.rcParams.update({\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'font.size': 18,\n",
    "    'font.weight': 'normal',\n",
    "    'axes.labelsize': 18,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'xtick.labelsize': 18,\n",
    "    'ytick.labelsize': 18,\n",
    "    'legend.fontsize': 14,\n",
    "})\n",
    "\n",
    "# Function to extract the metric values and standard deviations\n",
    "def extract_values_with_std(cell_value):\n",
    "    value, std = cell_value.split(' Â± ')\n",
    "    return float(value), float(std)\n",
    "\n",
    "# Create the grouped 2x2 plot\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "target_columns = [\"r_dicho\", \"b_dicho\"]\n",
    "metric_cols = ['AUC-ROC', 'AUPR']\n",
    "xlabels = ['a', 'b', 'c', 'd'] \n",
    "\n",
    "tmp_data = pd.DataFrame()\n",
    "\n",
    "ax_index = 0\n",
    "for tr in target_columns:\n",
    "\n",
    "    file_name = f'{path}/{tr}_mean'\n",
    "\n",
    "    # Read the data from the CSV file\n",
    "    data = pd.read_csv(f'{file_name}.csv')\n",
    "\n",
    "       \n",
    "    for col in metric_cols:\n",
    "        data[col], data[col + ' std'] = zip(*data[col].map(extract_values_with_std))\n",
    "    \n",
    "    # Extract the necessary columns for plotting\n",
    "    alg_names = data['Alg'].unique()\n",
    "\n",
    "    \n",
    "    # Create grouped plots for each metric in the 4x2 grid\n",
    "    for j, metric in enumerate(metric_cols):   \n",
    "\n",
    "        row = ax_index // 2\n",
    "        col = ax_index % 2\n",
    "        ax = axs[row, col]\n",
    "\n",
    "        width = 0.4\n",
    "        r = width/2\n",
    "        x = np.arange(len(alg_names))\n",
    "        \n",
    "        ax.bar(x - width/2, data[data['Scaling'] == 'MinMax'][metric], width, label='MinMax', color='skyblue', edgecolor='black')\n",
    "        ax.bar(x + width/2, data[data['Scaling'] == 'Standard'][metric], width, label='Standard', color='orange', edgecolor='black')\n",
    "\n",
    "        # Add error bars for standard deviation\n",
    "        ax.errorbar(x - width/2, data[data['Scaling'] == 'MinMax'][metric],\n",
    "                    yerr=data[data['Scaling'] == 'MinMax'][metric + ' std'],\n",
    "                    fmt='none', ecolor='black', capsize=3, elinewidth=1, marker='_', markersize=6)\n",
    "        ax.errorbar(x + width/2, data[data['Scaling'] == 'Standard'][metric],\n",
    "                    yerr=data[data['Scaling'] == 'Standard'][metric + ' std'],\n",
    "                    fmt='none', ecolor='black', capsize=3, elinewidth=1, marker='_', markersize=6)\n",
    "        \n",
    "        ax.set_xlabel(f'({xlabels[ax_index]})')\n",
    "        ax.set_ylabel('Score (%)')\n",
    "        ax.set_title(metric_cols[j])\n",
    "        \n",
    "        # Set the ylim based on stds\n",
    "        min_val = min(min(data[data['Scaling'] == 'MinMax'][metric] - 2 * data[data['Scaling'] == 'MinMax'][metric + ' std']),\n",
    "                    min(data[data['Scaling'] == 'Standard'][metric] - 2 * data[data['Scaling'] == 'Standard'][metric + ' std']))\n",
    "        \n",
    "        max_val = max(max(data[data['Scaling'] == 'MinMax'][metric] + 2 * data[data['Scaling'] == 'MinMax'][metric + ' std']),\n",
    "                    max(data[data['Scaling'] == 'Standard'][metric] + 2 * data[data['Scaling'] == 'Standard'][metric + ' std']))\n",
    "        ax.set_ylim(min_val, max_val)\n",
    "\n",
    "        # Show algorithm names below the bars\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(alg_names, ha='center')\n",
    "\n",
    "        # Set the legend for the grouped bars\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "        ax_index += 1\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as an image file for publication\n",
    "plt.savefig(f'{path}/bar_plot.png', bbox_inches='tight', facecolor='white', dpi=120)\n",
    "\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating Feature Ranking Mean\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "results_path = \"Results/2_Prepared_Data\"\n",
    "random_state_numbers = [7, 42, 75, 101, 216]\n",
    "preprocessing_methods = [\"MinMax\", \"Standard\"]\n",
    "target_columns = ['r_dicho', 'b_dicho']\n",
    "nFolds = 5\n",
    "\n",
    "\n",
    "def get_finalScores(features_lists, features):\n",
    "    final_scores = pd.DataFrame(data = {'Features':features})\n",
    "    for i in range(len(features_lists)):\n",
    "        tmp_scores = []\n",
    "        for feature in final_scores[\"Features\"]:\n",
    "            tmp_scores.append(features_lists[i].index(feature)+1)\n",
    "        final_scores[i+1] = tmp_scores\n",
    "\n",
    "    modes = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    for i in range(len(final_scores)):\n",
    "        data = final_scores.iloc[i, 1:]\n",
    "        modes.append(data.mode()[0])\n",
    "        means.append(data.mean())\n",
    "        stds.append(data.std())\n",
    "    final_scores['Mode'] = modes\n",
    "    final_scores['Mean'] = means\n",
    "    final_scores['Std'] = stds\n",
    "    \n",
    "    final_scores = final_scores.sort_values(by=['Mode', 'Mean', \"Std\"])\n",
    "    final_scores.reset_index(drop=True, inplace=True)\n",
    "    return final_scores\n",
    "\n",
    "def get_aggregatedScores(features_lists, features):\n",
    "    final_scores = pd.DataFrame(data = {'Features':features})\n",
    "    for key, value in features_lists.items():\n",
    "        tmp_scores = []\n",
    "        for feature in final_scores[\"Features\"]:\n",
    "            tmp_scores.append(value.index(feature)+1)\n",
    "        final_scores[key] = tmp_scores\n",
    "\n",
    "    final_scores = final_scores.sort_values(by=list(features_lists.keys())[0])\n",
    "    final_scores.reset_index(drop=True, inplace=True)\n",
    "    return final_scores\n",
    "\n",
    "agg_list = {}\n",
    "for tr in target_columns:\n",
    "    for pp in preprocessing_methods:\n",
    "        features_list = []\n",
    "        for rs in random_state_numbers:\n",
    "            tmp_path = f'{results_path}/{tr}/{rs}_{pp}/ranked_features'\n",
    "\n",
    "            for i in range(1, nFolds+1):\n",
    "                tmp_rankedFeatures = pd.read_csv(f'{tmp_path}/KBest_fold_{i}.csv')\n",
    "                features_list.append(tmp_rankedFeatures[\"Features\"].tolist())\n",
    "\n",
    "        final_df = get_finalScores(features_list, features_list[0])        \n",
    "        final_df.to_csv(f\"{path}/{tr}_KBest_rankedFeatures_{pp}.csv\", index=False)\n",
    "        \n",
    "        agg_list[f'{tr}_{pp}'] = final_df['Features'].tolist()\n",
    "\n",
    "agg_dfs = get_aggregatedScores(agg_list, list(agg_list.values())[0])\n",
    "tmp_features = agg_dfs[\"Features\"].tolist()\n",
    "agg_dfs.to_csv(f\"{path}/final_KBest_rankedFeatures.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
